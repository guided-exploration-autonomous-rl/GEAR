
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->
<script src="load-mathjax.js" async></script>


<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:1.2cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
	text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@marceltornev">
        <meta name="twitter:title" content="Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://www.linkedin.com/in/max-balsells">Max Balsells<sup>1*</sup></a>,
                <a href="https://marceltorne.github.io/">Marcel Torne<sup>2*</sup></a>,
                <a href="">Zihan Wang<sup>1*</sup></a>,
                <a href="">Samedh Desai<sup>1</sup></a>,
                <a href="https://people.eecs.berkeley.edu/~pulkitag/">Pulkit Agrawal<sup>2</sup></a>,
                <a href="https://abhishekunique.github.io">Abhishek Gupta<sup>1</sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> University of Washington</span>
            <span><sup>2</sup> Massachusetts Institute of Technology </span>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><b>arXiv 2023 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="paper-btn" href="TODO">
                <span class="material-icons"> code </span>
                Code
            </a>
            </div>
        </div>
    </div>

    <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="100%" autoplay playsinline controls>
                    <source src="materials/explanation.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section>
    
    <section id="results-image">
        <center>
            <figure>
                <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/collage_gear.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section>


    
    <section id="abstract"/>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                Ideally, we would place a robot in a real-world environment and leave it there improving on its own by gathering more experience autonomously. However, algorithms for autonomous robotic learning have been challenging to realize in the real world. While this has often been attributed to the challenge of sample complexity, even sample-efficient techniques are hampered by two major challenges - the difficulty of providing well ``shaped" rewards, and the difficulty of continual reset-free training. In this work, we describe a system for real-world reinforcement learning that enables agents to show continual improvement by training directly in the real world without requiring painstaking effort to hand-design reward functions or reset mechanisms. Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration while leveraging a simple self-supervised learning algorithm for goal-directed policy learning. We show that in the absence of resets, it is particularly important to account for the current "reachability" of the exploration policy when deciding which regions of the space to explore. Based on this insight, we instantiate a practical learning system - GEAR, which enables robots to simply be placed in real-world environments and left to train autonomously without interruption.  The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of binary comparative feedback.  We evaluate this system on a suite of robotic tasks in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. 
            </p>
        </div>
    </section>
    <section id="method"/>
        <hr>
        <h2>Method</h2>
            <center>
                <section id="teaser-image">
                    <center>
                        <!-- <figure>
                            <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                                <source src="materials/huge_detailed++6.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </figure> -->

                        <figure>
                            <a>
                                <img width="100%" src="materials/method.png">
                            </a>
                            <p class="caption">
                                Problem setting in GEAR. The robot explores the world autonomously and reset-free only using cheap, occasional binary feedback from non-expert users to guide exploration. This allows for massive scaling of data experience and solving of much more challenging tasks.
                            </p> <br>
            
                    </center>
                </section>
            </center>
            <!--
                <h3>Goal Selector Learning from Human Preferences</h3>
                        <div class="flex-row">
                            
                            <p> 
                                Existing diffusion models are often trained on massive datasets with a vast amounts of computational resources. In this paper, we explore and present tools on how we may utilize probabilistic composition of as an algebra to repurpose diffusion models, <b>without any finetuning</b>, for variety of downstream tasks.  
                                <br>
                                <br>
                                Consider two probability distributions $q^1(x)$ and $q^2(x)$, each represented with a different diffusion model.  Can we draw samples from the product distribution $q^{\textup{prod}}(x) \propto q^1(x)q^2(x)$ specified by each diffusion model? One potential solution is to note that the diffusion process encodes the noisy gradients of each distribution, letting us use the <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">sum of the scores of each diffusion process</a> to compose these models. While this approach can be effective, it is not completely mathematically accurate. To correctly sample from a reverse diffusion corresponding to $q^{\textup{prod}}(x)$, at each noise timestep $t$, we must compute the score:
                                \[ \nabla_{x}\log \tilde{q}_{t}^{\textup{ prod}}(x_t) = \nabla_x\log  \left(\int
                dx_{0}
                q^{1}(x_{0})q^2(x_{0})~ q(x_t|x_{0})\right).\]
                                Directly summing up the predicted scores of each separate diffusion model instead gives us the score:
                                \[ \nabla_{x}\log q_{t}^{\textup{prod}}(x_t)
                = 
                \nabla_{x}\log  \left(\int dx_0 q^1(x_0)q(x_t|x_0)\right)+\nabla_{x}\log  \left(\int dx_0 q^2(x_0)q(x_t|x_0)\right).\]
                                When the $t > 0$, the above expressions are not equal, and thus sample from the incorrect reverse diffusion process for $q^{\textup{prod}}(x)$. To address this theoretical issue, we propose two methodological contributions to properly sample across a set of different compositions of diffusion models (see our paper for analysis of why other forms of composition fail):
                            </p>
                            <p>
                                <b>Sampling from Composed Diffusion Models using Annealed MCMC:</b> While the score estimate $\nabla_{x}\log q_{t}^{\textup{prod}}(x_t)$ does not correspond to the correct score estimate necessary to sample from the reverse diffusion process for $q_{t}^{\textup{prod}}(x_t)$, it does define a unnormalized probability distribution (EBM) at timestep t. The sequence of score estimates across different time points can then be seen as defining an annealed sequence of distributions starting from Gaussian
                noise and evolving to our desired distribution $q_{t}^{\textup{prod}}(x_t)$.
                                Thus, we may still sample from $q_{t}^{\textup{prod}}(x_t)$ using <b>Annealed MCMC</b> procedure, where we initialize a sample from
                                Gaussian noise, and draw samples sequentially across different intermediate distributions by running multiple steps of MCMC
                                sampling initialized from samples from the previous distribution. 
                            </p>
                            <p>
                                <b>Energy Based Diffusion Parameterization:</b> In practice, MCMC sampling does not correctly sample from underlying distribution without a Metropolis Adjustment step. With the typical score parameterization of diffusion models,
                                this is not possible, as there is no unnormalized density associated with the score field. Instead, we propose
                                to use an energy based parameterization of diffusion model, where at each timestep, our neural network predicts an <b>scalar energy value for each data point</b> and the utilizes the gradient of the energy with respect to the input as the score for the diffusion process. The predicted energy gives us an <b>unnormalized estimate of
                                the probability density</b>, enabling us to use Metropolis Adjustment in sampling. We further show that
                                the unnormalized estimate of the density enables us to do additional compositions with a diffusion model.
                            </p>
                            <p>
                                Below, we demonstrate results illustrating how we may use the above tools to re-purpose diffusion models in a variety of different settings. 
                            </p>
                        </div>
            -->
        
    </section>


    <section id="results">
        <hr>
        <h2>Real World Experiments</h2>
        <figure>
            <a>
                <img width="100%" src="materials/real_world.png">
            </a>
            <p class="caption">
                Success ratio of GEAR when evaluating the policies on the tasks conducted in the real world.
            </p> <br>
        </figure>
        <h2>Simulation Experiments</h2>  
            <figure>
                <a>
                    <img width="100%" src="materials/figures_inline.png">
                </a>
                <p class="caption">
                    The 4 simulation benchmarks where we test GEAR comparing it against other baselines. <strong>Kitchen</strong>, and <strong>Pusher</strong>, are manipulation tasks; <strong>Four rooms</strong> and <strong>Navigation</strong> are 2D navigation tasks
                </p> <br>
            </figure>
            <figure>
                <a>
                    <img width="100%" src="materials/curves.png">
                </a>
                <p class="caption">
                    Success curves of GEAR on the proposed benchmarks compared to the baselines. 
                    GEAR outperforms the rest of the baselines, which cannot solve most of the environments. Furthermore, we see that the approach using an autoregressive model performs as well as the using oracle densities. 
                    Some curves may not seem visible because they never succeeded and hence stay at 0. 
                    Note: Due to the large observation space in the kitchen environment, GEAR could only be run with the autoregressive model (there is no oracle densities curve for that experiment). The curves are the average of 4 runs, and the shaded region corresponds to the standard deviation. 
                </p> <br>
            </figure>
            

        <h2>Crowdsourced Experiments</h2>
        <figure>
            <a>
                <img width="100%" src="materials/crowdsourcing.png">
            </a>
            <p class="caption">
                <strong>Comparing performance in Four Rooms with different types of human feedback. We see that GEAR manages to succeed with any type of annotation.
            </p> <br>
        </figure>
        
    </section> 
    
    <!-- <section id="robust_noise">
        <hr>
        <h2>Robustness to Learning from Noisy Human Feedback</h2>
        <center>
        <figure>
            <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                <source src="materials/huge_explained_rl.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>

    </section> -->

    <section id="analysis">
        <hr>
        <h2>Reachability</h2>
        <figure>
            <a>
                <img width="100%" src="materials/huge_vs_gear.png">
            </a>
            <p class="caption">
                We see that, by accounting for reachability, GEAR manages to command meaningful goals, i.e. is able to command subgoals that it knows how to reach. We see that other methods like HuGE fail to do so, which translates to poorer performance.
            </p> <br>
        </figure>        
        <hr>

    </section> 

    <!--
    <section id="paper">

        <h2>Team</h2>        
        <div class="row">
            <div class="column5">
                <a href='https://marceltorne.github.io/'>
                    <img  src=./materials/people/marcel.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Marcel Torne Villasevil </p>
                <p class=institution>Harvard University, MIT</p>
            </div>

            <div class="column5">
                <a href=''>
                    <img  src=./materials/people/max.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Max Balsells i Pamies </p>
                <p class=institution>University of Washington</p>
            </div>

            <div class="column5">
                <a href='https://taochenshh.github.io'>
                    <img  src=./materials/people/tao.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Tao Chen </p>
                <p class=institution>MIT</p>
            </div>
            <div class="column5">
                <a href='https://people.eecs.berkeley.edu/~pulkitag/'>
                    <img  src=./materials/people/pulkit.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                </a>
                <p class=profname> Pulkit Agrawal </p>
                <p class=institution>MIT</p>
            </div>



            <center>
                <div class="column0">
                    <a href="https://abhishekunique.github.io">
                        <img src=./materials/people/abhishek.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                    </a>
                    <p class=profname> Abhishek Gupta </p>
                    <p class=institution>University of Washington</p>
                </div>

                <div class="column0">
                    <a href='https://people.eecs.berkeley.edu/~pulkitag/'>
                        <img  src=./materials/people/pulkit.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                    </a>
                    <p class=profname> Pulkit Agrawal </p>
                    <p class=institution>MIT</p>
                </div>
    
                <div class="column0">
                    <a href="https://abhishekunique.github.io">
                        <img src=./materials/people/abhishek.png class="figure-img img-fluid rounded-circle" height=200px width=200px>
                    </a>
                    <p class=profname> Abhishek Gupta </p>
                    <p class=institution>University of Washington</p>
                </div>

            </center>

         </div>

    </section>



    -->

   
    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
    </section>
    


</div>
</body>
</html>
